## *What generative AI changes, and what it does not*

One of the most active debates surrounding generative AI in education concerns **cognitive offloading**: the use of external tools to reduce the mental effort required to complete a task. Offloading itself is not new. Writing, calculators, dictionaries, and search engines have long functioned as cognitive supports. What is new is the **scope, fluency, and apparent completeness** of generative AI, which can now perform complex linguistic and analytical tasks end-to-end.

This shift has prompted concerns about **deskilling** or **cognitive atrophy**, the possibility that frequent reliance on AI may weaken foundational skills when tools replace, rather than support, formative practice. Research in this area is still emerging, and conclusions remain provisional. What does appear consistently, however, is that learning outcomes depend less on the presence of tools than on **where difficulty is located** and **what kinds of effort learners are asked to sustain**.

In learning research, this distinction is often discussed through the lens of _desirable difficulties_ and _disuse_. Some forms of difficulty, such as retrieval, effortful recall, and the need to generate language under constraint, are not obstacles to learning but conditions for it. Other forms of difficulty, such as unnecessary complexity, time pressure, or procedural friction, do little to support learning and may even interfere with it. When learners routinely bypass effortful cognitive work, skills that depend on use and reinforcement may weaken over time.

For World Languages & Cultures, these concerns are especially salient. Language learning is not primarily about producing correct output; it is about **internalization, retrieval, timing, repair, and judgment**. Many of the skills language courses aim to develop (lexical access, grammatical intuition, pragmatic sensitivity) depend on repeated, effortful use. When generative AI performs these operations on a student's behalf, the risk is not academic dishonesty so much as **removing the conditions under which those skills develop**.

At the same time, framing the issue as simple "overuse" misses an important distinction. Cognitive offloading can function in different ways. In some instructional designs, AI supports higher-order thinking by providing material for comparison, critique, or revision. In others, it substitutes directly for practice that is central to learning. The difference lies not in the tool itself, but in how instructional tasks allocate responsibility for thinking, decision-making, and revision.

This distinction aligns with broader work on distributed cognition. Tools can extend learning productively when learners remain responsible for interpretation and judgment. Problems arise when tools absorb the very cognitive work learners are meant to practice. From this perspective, concerns about deskilling are best understood not as moral failings on the part of students, but as **design risks** that instructors can mitigate, or unintentionally amplify, through assignment structure.

For language educators, this leads to several practical insights. Foundational activities, including early writing, spontaneous speech, basic translation, and retrieval-based practice, are particularly vulnerable to premature offloading. Later-stage or advanced activities, where the learning goal shifts toward analysis, interpretation, or meta-awareness, may be more resilient and may even benefit from carefully constrained AI use. This helps explain why some instructors choose **purposeful non-use** of AI in certain contexts: not as a rejection of technology, but as a recognition that some kinds of learning require sustained human effort.

The research conversation remains open. We do not yet have long-term evidence about how extensive AI-supported learning affects language development over time. What we do have are strong reasons to avoid both extremes: uncritical adoption that displaces essential practice, and blanket prohibition that forecloses thoughtful experimentation. A stance of **measured, revisable judgment**, grounded in learning goals rather than tool capabilities, remains the most responsible response.

Concerns about cognitive offloading, then, do not require panic or prohibition. They invite clarity: about which difficulties are desirable, which forms of effort matter most, and how instructional design can preserve those conditions even in an AI-rich learning environment.

---

## Design Heuristics: Preserving Learning in an AI-Rich Environment

> _A practical way to think about cognitive offloading is to ask:_ **If AI is doing X, what cognitive work should students still be doing themselves?**

**If AI generates language…** → Students should _analyze, revise, justify, or contextualize_ that language.

**If AI summarizes or explains content…** → Students should _evaluate accuracy, identify omissions, or connect ideas to course concepts or lived context_.

**If AI produces a first draft…** → Students should _document revision decisions_: what they changed, why, and for whom.

**If AI speeds up production…** → Students should _slow down interpretation, reflection, or discussion_.

**If AI removes friction from a task…** → Ask whether that friction was **logistical** (formatting, repetition, time pressure) or **formative** (retrieval, practice, judgment).

- Reduce the former.
- Protect the latter.

**If AI replaces a step entirely…** → Reconsider whether that step is essential for skill development at this course level.

**If AI use feels especially tempting for students…** → Examine whether the assignment primarily rewards _completion_ or _learning over time_.

---

### How to Use These Heuristics

These heuristics are not rules or requirements. They are prompts for instructional design: to decide where AI use aligns with learning goals, to identify activities where non-use is pedagogically justified, and to redesign assignments so essential cognitive work remains visible.

Used this way, generative AI becomes a **design lens rather than a shortcut**, and concerns about deskilling become opportunities for clearer alignment between effort, difficulty, and learning.

---
### Bibliograpahy

Bjork, E. L., & Bjork, R. A. (2023). Introducing desirable difficulties into practice and instruction: Obstacles and opportunities. researchgate.net. Retrieved from https://www.researchgate.net/profile/Logan-Fiorella/publication/369588588_Learning_by_Teaching/links/64234a8ca1b72772e431adec/Learning-by-Teaching.pdf#page=26

Gannon, Kevin. “Why Calls for a ‘Return to Rigor’ Are Wrong: What’s the Point of Pursuing ‘Solutions’ That Exacerbate Student Disengagement, the Very Problem They Are Supposed to Solve?” The Chronicle of Higher Education, 2023.

Gerlich, M. (2025). AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical Thinking. _Societies_, _15_(1), 6. https://doi.org/10.3390/soc15010006

de Bruin, A.B.H., Biwer, F., Hui, L. _et al._ Worth the Effort: the Start and Stick to Desirable Difficulties (S2D2) Framework. _Educ Psychol Rev_ **35**, 41 (2023). https://doi.org/10.1007/s10648-023-09766-w

Jain, N., & Kiran, M. (2025). Rethinking Education in the Age of Generative AI: Cognitive Offloading, Assessment Reform, and Institutional Adaptation.